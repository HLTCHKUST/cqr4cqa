# modified from https://github.com/lvwerra/trl/blob/b5c1df4ebdcca4873af906ab9d28436e4e4ef51b/trl/ppo.py
# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02-ppo.ipynb (unless otherwise specified).

__all__ = ['AdaptiveKLController', 'FixedKLController', 'PPOTrainer']

import time

import numpy as np
from transformers import AdamW, get_scheduler
import torch
from torch.nn import CrossEntropyLoss
from torch.cuda.amp import autocast, GradScaler

from src.utils.ppo_utils import (logprobs_from_logits,
                         whiten,
                         clip_by_value,
                         entropy_from_logits,
                         flatten_dict,
                         stats_to_np,
                         stack_dict_batches)


class AdaptiveKLController:
    """
    Adaptive KL controller described in the paper:
    https://arxiv.org/pdf/1909.08593.pdf
    """
    def __init__(self, init_kl_coef, target, horizon):
        self.value = init_kl_coef
        self.target = target
        self.horizon = horizon

    def update(self, current, n_steps):
        target = self.target
        proportional_error = np.clip(current / target - 1, -0.2, 0.2)
        mult = 1 + proportional_error * n_steps / self.horizon
        self.value *= mult


class PPOTrainer:
    """
    The PPO_trainer uses Proximal Policy Optimization to optimise language models.
    """
    def __init__(self, model, ref_model, args):
        """
        
        Initialize PPOTrainer.

        Args:
            model (torch.model): Hugging Face transformer GPT2 model with value head
            ref_model (torch.model): Hugging Face transformer GPT2 refrence model used for KL penalty
            args: should contain PPO parameters for training. Can include following keys:
                'lr' (float): Adam learning rate, default: 1.41e-5
                'batch_size' (int): Number of samples per optimisation step, default: 256
                'ppo_epochs' (int): Number of optimisation epochs per batch of samples, default: 4
                'gamma' (float)): Gamma parameter for advantage calculation, default: 1.
                'lam' (float): Lambda parameter for advantage calcualation, default: 0.95
                'cliprange_value' (float): Range for clipping values in loss calculation, default: 0.2
                'cliprange' (float): Range for clipping in PPO policy gradient loss, default: 0.2
                'vf_coef' (float): Scaling factor for value loss, default: 0.1
                'adap_kl_ctrl' (bool): Use adaptive KL control, otherwise linear, default: True
                'init_kl_coef' (float): Initial KL penalty coefficient (used for adaptive and linear control), default: 0.2
                'target' (float): Target KL value for adaptive KL control, default: 6.0
                'horizon' (float): Horizon for adaptive KL control, default: 10000
                'grad_accum' (int): Gradient accumulation
        """
        self.args = args
        self.ref_model = ref_model
        self.model = model
        self.counter = 0
        self.grad_accum = self.args.grad_accum
        no_decay = ["bias", "LayerNorm.weight"]
        optimizer_grouped_parameters = [
                {
                    "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
                    "weight_decay": self.args.weight_decay,
                },
                {
                    "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
                    "weight_decay": 0.0
                },
            ]
        self.optimizer = AdamW(optimizer_grouped_parameters, lr=self.args.lr, eps=self.args.adam_epsilon)

        if self.args.use_lr_scheduler:
            self.scheduler = get_scheduler(
                self.args.lr_scheduler_type,
                self.optimizer,
                num_warmup_steps=self.args.warmup_steps,
                num_training_steps=self.args.num_training_steps,
            )
        else:
            self.scheduler = None
        
        # FP16 scaler
        if self.args.fp16:
            self.scaler = GradScaler()
        else:
            self.scaler = None
                
        # Adaptive KL
        self.kl_ctl = AdaptiveKLController(self.args.init_kl_coef,
                                           self.args.target,
                                           self.args.horizon)

    def step(self, model_input, position_ids, attention_mask, token_type_ids, gen_len, scores, 
                 decoder_input_ids=None, decoder_attention_mask=None, num_ce_tokens=0, ce_labels=None):
        """
        Run a PPO optimisation step.

        args:
            query (torch.tensor): tensor containing the encoded queries, shape [batch_size, query_length]
            response (torch.tensor): tensor containing the encoded responses, shape [batch_size, response_length]
            scores (torch.tensor): tensor containing the scores, shape [batch_size]

        returns:
            train_stats (dict): a summary of the training statistics
        """

        bs = self.args.batch_size
        timing = dict()
        t0 = time.time()

        logprobs, ref_logprobs, values = self.batched_forward_pass(model_input, position_ids, attention_mask, token_type_ids, gen_len, 
                                                                       decoder_input_ids, decoder_attention_mask, num_ce_tokens)
        timing['time/ppo/forward_pass'] = time.time()-t0 

        t = time.time()
        rewards, non_score_reward, kl_coef = self.compute_rewards(scores, logprobs, ref_logprobs, 
                                                    attention_mask[:, -gen_len-1:-1] if decoder_attention_mask is None else decoder_attention_mask)
        timing['time/ppo/compute_rewards'] = time.time()-t
        
        t = time.time()
        all_stats = []
        idxs = list(range(bs))
        for _ in range(self.args.ppo_epochs):
            train_stats = self.train_minibatch(logprobs, values, rewards, model_input, position_ids, attention_mask, token_type_ids, 
                                                gen_len, decoder_input_ids, decoder_attention_mask, num_ce_tokens, ce_labels)
            all_stats.append(train_stats)
                
        timing['time/ppo/optimize_step'] = time.time()-t

        t = time.time()
        train_stats = stack_dict_batches(all_stats)
        # train_stats = stack_dicts(all_stats)
        
        # reshape advantages/ratios such that they are not averaged.
        try:
            train_stats['policy/advantages'] = torch.flatten(train_stats['policy/advantages']).unsqueeze(0)
            train_stats['policy/ratio'] = torch.flatten(train_stats['policy/ratio']).unsqueeze(0)
        except TypeError as e:
            train_stats['policy/advantages'] = None
            train_stats['policy/ratio'] = None

        stats = self.record_step_stats(scores=scores, logprobs=logprobs, ref_logprobs=ref_logprobs,
                                       non_score_reward=non_score_reward, train_stats=train_stats,
                                       kl_coef=kl_coef)
        stats = stats_to_np(stats)
        timing['time/ppo/calc_stats'] = time.time()-t

        self.kl_ctl.update(stats['objective/kl'], self.args.batch_size)

        timing['time/ppo/total'] = time.time()-t0
        stats.update(timing)
        return stats

    def batched_forward_pass(self, model_input, position_ids, attention_mask, token_type_ids, gen_len, 
                                     decoder_input_ids, decoder_attention_mask, num_ce_tokens):
        """Calculate model outputs in multiple batches."""
        with torch.no_grad():
                output = self.model.forward_value(model_input, position_ids=position_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask)
                ref_logits = self.ref_model(model_input, position_ids=position_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask)["logits"]
        logits, v = output["logits"], output["values"]   
        if decoder_input_ids is None:
            value = v[:, -gen_len-1:-1]
            logprob = logprobs_from_logits(logits[:,-gen_len-1:-1,:], model_input[:,-gen_len:])
            ref_logprob = logprobs_from_logits(ref_logits[:,-gen_len-1:-1,:], model_input[:,-gen_len:])

            value[attention_mask[:, -gen_len-1:-1] == 0] = 0 # Zero out
            logprob[attention_mask[:, -gen_len-1:-1] == 0] = 0 # Zero out
            ref_logprob[attention_mask[:, -gen_len-1:-1] == 0] = 0 # Zero out
        else:
            value = v[:, :-1]
            logprob = logprobs_from_logits(logits[:,:-1,:], decoder_input_ids[:,1:])
            ref_logprob = logprobs_from_logits(ref_logits[:,:-1,:], decoder_input_ids[:,1:])

            value[decoder_attention_mask[:,:-1] == 0] = 0 # Zero out
            logprob[decoder_attention_mask[:,:-1] == 0] = 0 # Zero out
            ref_logprob[decoder_attention_mask[:,:-1] == 0] = 0 # Zero out
            
        # Handle CE for MIXER
        if num_ce_tokens > 0:
            value[:,:num_ce_tokens] = 0
            logprob[:,:num_ce_tokens] = 0
            ref_logprob[:,:num_ce_tokens] = 0        
        
        return logprob, ref_logprob, value

    def train_minibatch(self, logprobs, values, rewards, model_input, position_ids, attention_mask, token_type_ids, gen_len, decoder_input_ids, decoder_attention_mask, num_ce_tokens, ce_labels):
        """Train one PPO minibatch"""
        
        if self.scaler is not None:
            with autocast():
                loss_p, loss_v, train_stats = self.loss(logprobs, values, rewards, model_input, position_ids, attention_mask, token_type_ids, 
                                                            gen_len, decoder_input_ids, decoder_attention_mask, num_ce_tokens, ce_labels)
                loss = loss_p + loss_v
                
            # Backward pass
            self.scaler.scale(loss).backward()

            # scaler.step() first unscales the gradients of the optimizer's assigned params.
            # If these gradients do not contain infs or NaNs, optimizer.step() is then called, otherwise, optimizer.step() is skipped.
            self.scaler.step(self.optimizer)

            # Updates the scale for next iteration.
            self.scaler.update()
        else:
            loss_p, loss_v, train_stats = self.loss(logprobs, values, rewards, model_input, position_ids, attention_mask, token_type_ids, 
                                                        gen_len, decoder_input_ids, decoder_attention_mask, num_ce_tokens, ce_labels)
            loss = loss_p + loss_v
        
        if self.grad_accum > 1:
            loss = loss / self.grad_accum
        loss.backward()
        self.update_model()
    
        return train_stats

    def compute_rewards(self, scores, logprobs, ref_logprobs, gen_attn_mask):
        """Compute per token rewards from scores and KL-penalty."""
        """Compute modified reward R(x,y) as explained in eq (2) | https://arxiv.org/pdf/1909.08593.pdf"""
        num_right_masks = (gen_attn_mask == 0).sum(dim=-1)
        kl = logprobs - ref_logprobs
        non_score_reward = -self.kl_ctl.value * kl
        rewards = non_score_reward.clone().detach()

        for i, num_right_mask in enumerate(num_right_masks):
            if num_right_mask != 0:
                rewards[i, -num_right_mask-1] += scores[i]
            else:
                rewards[:, -1] += scores[i]
            
        return rewards, non_score_reward, self.kl_ctl.value

    def loss(self, old_logprobs, values, rewards, model_input, position_ids, attention_mask, token_type_ids, 
                 gen_len, decoder_input_ids, decoder_attention_mask, num_ce_tokens, ce_labels):
        """Calculate policy and value losses."""
        lastgaelam = 0
        advantages_reversed = []
        
        t_adv = time.time()
        
        # Implementation of eq (11) and eq (12) from PPO paper
        with torch.no_grad():
            nextvalues = values.roll(-1, dims=-1)
            nextvalues[:,-1] = 0
            delta = rewards + self.args.gamma * nextvalues - values
            for t in reversed(range(gen_len)):
                # nextvalues = values[:, t + 1] if (t < gen_len) else 0.0
                # delta = rewards[:, t] + self.args.gamma * nextvalues - values[:, t]
                if t < num_ce_tokens:
                    # Handle CE from Mixer
                    advantages_reversed.append(torch.zeros(values.shape[0], dtype=torch.float, device=values.device))
                else:
                    lastgaelam = delta[:, t] + self.args.gamma * self.args.lam * lastgaelam
                    advantages_reversed.append(lastgaelam)
            advantages = torch.stack(advantages_reversed[::-1]).transpose(0, 1)

            returns = advantages + values
            advantages = whiten(advantages)
        
        # forward up to before <bos> without gradient
        if decoder_input_ids is None:
            with torch.no_grad():
                    output = self.model.forward_value(model_input[:,:-gen_len-1], position_ids=position_ids[:,:-gen_len-1], 
                                    attention_mask=attention_mask[:,:-gen_len-1], token_type_ids=token_type_ids[:,:-gen_len-1], use_cache=True)
            past_key_values = output["past_key_values"]

            output = self.model.forward_value(
                model_input[:,-gen_len-1:-1], past_key_values=past_key_values, 
                position_ids=position_ids[:,-gen_len-1:-1], attention_mask=attention_mask[:,:-1], token_type_ids=token_type_ids[:,-gen_len-1:-1]
            )

            logits, vpred = output["logits"], output["values"]
            logprob = logprobs_from_logits(logits, model_input[:,-gen_len:])

            #only the generation part of the values/logprobs is needed
            vpred[attention_mask[:, -gen_len-1:-1] == 0] = 0 # Zero out
            logprob[attention_mask[:, -gen_len-1:-1] == 0] = 0 # Zero out  
            
        else:
            output = self.model.forward_value(model_input, position_ids=position_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, 
                                      decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask)
            
            logits, vpred = output["logits"], output["values"]
            vpred = vpred[:, :-1]
            logprob = logprobs_from_logits(logits[:,:-1,:], decoder_input_ids[:,1:])

            #only the generation part of the values/logprobs is needed
            vpred[decoder_attention_mask[:,:-1] == 0] = 0 # Zero out
            logprob[decoder_attention_mask[:,:-1] == 0] = 0 # Zero out

        vpred[:, :num_ce_tokens] = 0 # CE Zero out
        vpredclipped = clip_by_value(vpred,
                                     values - self.args.cliprange_value,
                                     values + self.args.cliprange_value)

        # Equation 9 PPO Paper
        vf_losses1 = (vpred - returns)**2
        vf_losses2 = (vpredclipped - returns)**2
        
        # Clamped loss following DQL | https://discuss.pytorch.org/t/creating-a-clipped-loss-function/12022/4
        vf_loss = .5 * torch.mean(torch.clamp(torch.max(vf_losses1, vf_losses2), min=-1, max=1))
        # vf_loss = .5 * torch.mean(torch.max(vf_losses1, vf_losses2))
        vf_clipfrac =  torch.mean(torch.gt(vf_losses2, vf_losses1).double())

        ratio = torch.exp(logprob - old_logprobs)

        # Clipping surrogate (Section 6.1 PPO Paper)
        pg_losses = -advantages * ratio
        pg_losses2 = -advantages * torch.clamp(ratio,
                                               1.0 - self.args.cliprange,
                                               1.0 + self.args.cliprange)

        # Clamped loss following DQL | https://discuss.pytorch.org/t/creating-a-clipped-loss-function/12022/4
        pg_loss = torch.clamp(torch.max(pg_losses, pg_losses2), min=-1, max=1)
        # pg_loss = torch.max(pg_losses, pg_losses2)
        
        # Handle CE from MIXER
        ce_loss = torch.zeros(1, device=pg_loss.device)
        if num_ce_tokens > 0:            
            # Calculate CE
            ce_logits = logits[:,:num_ce_tokens].clone()

            ce_loss_fct = CrossEntropyLoss()
            ce_labels = ce_labels[:,:num_ce_tokens].to(ce_logits.device)
            ce_loss = ce_loss_fct(ce_logits.view(-1, ce_logits.shape[-1]), ce_labels.view(-1))
                
            # Replace pg_loss
            pg_loss[:,:num_ce_tokens] = ce_loss
            
        # Compute avg pg_loss to get a scalar loss
        pg_loss = torch.mean(pg_loss)

        with torch.no_grad():
            pg_clipfrac = torch.mean(torch.gt(pg_losses2, pg_losses).double())
            loss = pg_loss + (self.args.vf_coef * vf_loss)

            entropy = torch.mean(entropy_from_logits(logits))
            approxkl = .5 * torch.mean((logprob - old_logprobs)**2)
            policykl = torch.mean(logprob - old_logprobs)
            return_mean, return_var = torch.mean(returns), torch.var(returns)
            value_mean, value_var = torch.mean(values), torch.var(values)

        stats = dict(
            loss=dict(policy=pg_loss, value=vf_loss, ce_loss=ce_loss, total=loss),
            policy=dict(entropy=entropy, approxkl=approxkl,policykl=policykl, clipfrac=pg_clipfrac,
                        advantages=advantages, advantages_mean=torch.mean(advantages), ratio=ratio
            ),
            returns=dict(mean=return_mean, var=return_var),
            val=dict(vpred=torch.mean(vpred), error=torch.mean((vpred - returns) ** 2),
                     clipfrac=vf_clipfrac, mean=value_mean, var=value_var),
        )
        
        return pg_loss, self.args.vf_coef * vf_loss, flatten_dict(stats)


    def record_step_stats(self, kl_coef, **data):
        """Record training step statistics."""
        kl = data['logprobs'] - data['ref_logprobs']
        mean_kl = torch.mean(torch.sum(kl, axis=-1))
        mean_entropy = torch.mean(torch.sum(-data['logprobs'], axis=1))
        mean_non_score_reward =torch.mean(torch.sum(data['non_score_reward'], axis=1))
        stats = {
            'objective/kl': mean_kl,
            'objective/kl_dist': kl,
            'objective/logprobs': data['logprobs'],
            'objective/ref_logprobs': data['ref_logprobs'],
            'objective/kl_coef': kl_coef,
            'objective/entropy': mean_entropy,
            'ppo/mean_non_score_reward': mean_non_score_reward,
        }

        for k, v in data['train_stats'].items():
            try:
                stats[f'ppo/{k}'] = torch.mean(v, axis=0)
            except TypeError as e:
                stats[f'ppo/{k}'] = None
        try: 
            stats['ppo/val/var_explained'] = 1 - stats['ppo/val/error'] / stats['ppo/returns/var']
        except TypeError as e:
            stats['ppo/val/var_explained'] = None
        return stats
    
    def update_model(self):
        self.counter += 1
        if self.counter >= self.grad_accum:
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.max_grad_norm)
            self.optimizer.step()
            if self.scheduler is not None:
                self.scheduler.step()
            self.optimizer.zero_grad()
            self.counter = 0
            
    def is_update_step(self):
        return self.counter == self.grad_accum - 1